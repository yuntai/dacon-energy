{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#GPU memory usage:\n",
        "#23546MiB / 24263MiB\n",
        "\n",
        "#requirements:\n",
        "#pytorch-forecasting==0.9.0\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "import shutil\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from pytorch_forecasting.data import (\n",
        "    TimeSeriesDataSet,\n",
        "    GroupNormalizer\n",
        ")\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import (\n",
        "    ModelCheckpoint,\n",
        "    EarlyStopping,\n",
        "    LearningRateMonitor\n",
        ")\n",
        "from pytorch_forecasting.metrics import SMAPE\n",
        "from pytorch_forecasting.models import TemporalFusionTransformer\n",
        "\n",
        "\n",
        "# category columns\n",
        "CATE_COLS = ['num', \"mgrp\", 'holiday', 'dow', 'cluster', 'hot', 'nelec_cool_flag', 'solar_flag']\n",
        "\n",
        "# building cluster based on kmeans\n",
        "CLUSTER = {\n",
        "    0: [19, 20, 21, 49, 50, 51],\n",
        "    1: [1, 5, 9, 34],\n",
        "    2: [4, 10, 11, 12, 28, 29, 30, 36, 40, 41, 42, 59, 60],\n",
        "    3: [2, 3, 6, 7, 8, 13, 14, 15, 16, 17, 18, 22, 23, 24, 25, 26, 27, 31, 32, 33, 35, 37, 38, 39, 43, 44, 45, 46, 47, 48, 52, 53, 54, 55, 56, 57, 58],\n",
        "}\n",
        "\n",
        "# length of training data for prediction (5 weeks)\n",
        "ENCODER_LENGTH_IN_WEEKS = 5\n",
        "\n",
        "# learning rate determined by a cv run with train data less 1 trailing week as validation \n",
        "LRS = [0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306, 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.05099279397234306 , 0.05099279397234306, 0.05099279397234306, 0.05099279397234306,\n",
        "       0.005099279397234306, 0.005099279397234306, 0.005099279397234306, 0.005099279397234306,\n",
        "       0.005099279397234306, 0.005099279397234306, 0.005099279397234306, 0.005099279397234306,\n",
        "       0.005099279397234306, 0.0005099279397234307, 0.0005099279397234307, 0.0005099279397234307,\n",
        "       0.0005099279397234307, 0.0005099279397234307, 0.0005099279397234307]\n",
        "\n",
        "# number of epochs found in cv run\n",
        "NUM_EPOCHS = 66\n",
        "\n",
        "# number of seeds to use\n",
        "NUM_SEEDS = 10\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# hyper parameters determined by cv runs with train data less 1 trailing week as validation \n",
        "PARAMS = {\n",
        "    'gradient_clip_val': 0.9658579636307634,\n",
        "    'hidden_size': 180,\n",
        "    'dropout': 0.19610151695402608,\n",
        "    'hidden_continuous_size': 90,\n",
        "    'attention_head_size': 4,\n",
        "    'learning_rate': 0.08\n",
        "}\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--seed','-s', nargs='+', type=int, default=list(range(42, 42+NUM_SEEDS)))\n",
        "#parser.add_argument('--val', default=False, action='store_true')\n",
        "#parser.add_argument('--nepochs', '-e', type=int, default=NUM_EPOCHS)\n",
        "parser.add_argument('--gpu', type=int, default=0)\n",
        "parser.add_argument('--fit', default=False, action='store_true')\n",
        "parser.add_argument('--forecast', default=False, action='store_true')\n",
        "parser.add_argument('--dataroot', '-d', type=str, default=\"/data\")\n",
        "args = parser.parse_args()\n",
        "args.val = False\n",
        "args.nepochs = NUM_EPOCHS\n",
        "print(args)\n",
        "\n",
        "DATAROOT = Path(args.dataroot) # \ucf54\ub4dc\uc5d0 \u2018/data\u2019 \ub370\uc774\ud130 \uc785/\ucd9c\ub825 \uacbd\ub85c \ud3ec\ud568\n",
        "CKPTROOT = DATAROOT/\"ckpts\" # directory for model checkpoints\n",
        "CSVROOT = DATAROOT/\"csvs\" # directory for prediction outputs\n",
        "SUBFN = DATAROOT/\"sub.csv\" # final submission file path\n",
        "LOGDIR = DATAROOT/\"logs\" # pytorch_forecasting requirs logger\n",
        "\n",
        "def seed_all(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# prepare data features\n",
        "def __date_prep(df):\n",
        "\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "    df['hour'] = df['datetime'].dt.hour\n",
        "    df['dow'] = df['datetime'].dt.weekday\n",
        "    df['date'] = df['datetime'].dt.date.astype('str')\n",
        "    df['day'] = df['datetime'].dt.day\n",
        "    df['month'] = df['datetime'].dt.month\n",
        "\n",
        "    # FEATURE: saturday, sunday and speical holidays flagged as `holiday` flag\n",
        "    special_days = ['2020-06-06', '2020-08-15', '2020-08-17']\n",
        "    df['holiday'] = df['dow'].isin([5,6]).astype(int)\n",
        "    df.loc[df.date.isin(special_days), 'holiday'] = 1\n",
        "\n",
        "    # FEATURE: `hot` flag when the next day is holiday\n",
        "    hot = df.groupby('date').first()['holiday'].shift(-1).fillna(0).astype(int)\n",
        "    hot = hot.to_frame().reset_index().rename({'holiday': \"hot\"}, axis=1)\n",
        "    df = df.merge(hot, on='date', how='left')\n",
        "\n",
        "    # FEATURE: `cumhol` - how many days left in \uc5f0\ud734\n",
        "    h = (df.groupby('date').first()['holiday'] != 0).iloc[::-1]\n",
        "    df1 = h.cumsum() - h.cumsum().where(~h).ffill().fillna(0).astype(int).iloc[::-1]\n",
        "    df1 = df1.to_frame().reset_index().rename({'holiday': \"cumhol\"}, axis=1)\n",
        "    df = df.merge(df1, on='date', how='left')\n",
        "\n",
        "    return df\n",
        "\n",
        "# read data, process date and assign cluster number\n",
        "def __read_df():\n",
        "    train_columns = ['num','datetime','target','temperature','windspeed','humidity','precipitation','insolation','nelec_cool_flag','solar_flag']\n",
        "    test_columns = [c for c in train_columns if c != 'target']\n",
        "\n",
        "    train_df = pd.read_csv(DATAROOT/'train.csv', skiprows=[0], names=train_columns)\n",
        "    test_df = pd.read_csv(DATAROOT/'test.csv', skiprows=[0], names=test_columns)\n",
        "\n",
        "    __sz = train_df.shape[0]\n",
        "\n",
        "    df = pd.concat([train_df, test_df])\n",
        "\n",
        "    # assing cluster number to building\n",
        "    for k, nums in CLUSTER.items():\n",
        "        df.loc[df.num.isin(nums), 'cluster'] = k\n",
        "\n",
        "    df = __date_prep(df)\n",
        "\n",
        "    return df.iloc[:__sz].copy(), df.iloc[__sz:].copy()\n",
        "\n",
        "# add aggregate(mean) target feature for 'cluster', 'building', 'mgrp' per date\n",
        "def add_feats(df):\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    cols = ['target']\n",
        "    stats = ['mean']\n",
        "\n",
        "    # target null in test set to null for other columns care must be taken\n",
        "    g = df.groupby(['date', 'cluster'])\n",
        "    for s in stats:\n",
        "        col_mapper = {c:f\"{s}_{c}_cluster\" for c in cols}\n",
        "        tr = g[cols].transform(s).rename(col_mapper, axis=1)\n",
        "        df = pd.concat([df, tr], axis=1)\n",
        "\n",
        "    g = df.groupby(['date', 'num'])\n",
        "    for s in stats:\n",
        "        col_mapper = {c:f\"{s}_{c}_num\" for c in cols}\n",
        "        tr = g[cols].transform(s).rename(col_mapper, axis=1)\n",
        "        df = pd.concat([df, tr], axis=1)\n",
        "\n",
        "    g = df.groupby(['date', 'mgrp'])\n",
        "    for s in stats:\n",
        "        col_mapper = {c:f\"{s}_{c}_mgrp\" for c in cols}\n",
        "        tr = g[cols].transform(s).rename(col_mapper, axis=1)\n",
        "        df = pd.concat([df, tr], axis=1)\n",
        "\n",
        "    g = df.groupby(['date'])\n",
        "    for s in stats:\n",
        "        col_mapper = {c:f\"{s}_{c}\" for c in cols}\n",
        "        tr = g[cols].transform(s).rename(col_mapper, axis=1)\n",
        "        df = pd.concat([df, tr], axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "# interpolate NA values in test dataset\n",
        "def interpolate_(test_df):\n",
        "    # https://dacon.io/competitions/official/235736/codeshare/2844?page=1&dtype=recent\n",
        "    # \uc5d0\uc11c \uc81c\uc548\ub41c \ubc29\ubc95\uc73c\ub85c\n",
        "    __methods = {\n",
        "        'temperature': 'quadratic',\n",
        "        'windspeed':'linear',\n",
        "        'humidity':'quadratic',\n",
        "        'precipitation':'linear',\n",
        "        'insolation': 'pad'\n",
        "    }\n",
        "\n",
        "    for col, method in __methods.items():\n",
        "        test_df[col] = test_df[col].interpolate(method=method)\n",
        "        if method == 'quadratic':\n",
        "            test_df[col] = test_df[col].interpolate(method='linear')\n",
        "\n",
        "# prepare train and test data\n",
        "def prep():\n",
        "\n",
        "    train_df, test_df = __read_df()\n",
        "\n",
        "    # get nelec_cool_flag and solar_flag from training data\n",
        "    test_df = test_df.drop(['nelec_cool_flag','solar_flag'], axis=1)\n",
        "    test_df = test_df.merge(train_df.groupby(\"num\").first()[['nelec_cool_flag','solar_flag']].reset_index(), on=\"num\", how=\"left\")\n",
        "\n",
        "    # interpolate na in test_df for temperature, windspeed, humidity, precipitation & insolation\n",
        "    interpolate_(test_df)\n",
        "\n",
        "    # FEATURE(mgrp): group buildings having same temperature and windspeed measurements\n",
        "    s = train_df[train_df.datetime=='2020-06-01 00:00:00'].groupby(['temperature', 'windspeed']).ngroup()\n",
        "    s.name = 'mgrp'\n",
        "    mgrps = train_df[['num']].join(s, how='inner')\n",
        "\n",
        "    sz = train_df.shape[0]\n",
        "\n",
        "    df = pd.concat([train_df, test_df])\n",
        "    df = df.merge(mgrps, on='num', how='left')\n",
        "\n",
        "    # add aggregate target features\n",
        "    df = add_feats(df)\n",
        "\n",
        "    # add log target\n",
        "    df[\"log_target\"] = np.log(df.target + 1e-8)\n",
        "\n",
        "    for col in CATE_COLS:\n",
        "        df[col] = df[col].astype(str).astype('category')\n",
        "\n",
        "    # add time index feature\n",
        "    __ix = df.columns.get_loc('datetime')\n",
        "    df['time_idx'] = (df.loc[:, 'datetime'] - df.iloc[0, __ix]).astype('timedelta64[h]').astype('int')\n",
        "\n",
        "    train_df = df.iloc[:sz].copy()\n",
        "    test_df = df.iloc[sz:].copy()\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "# build traind datset\n",
        "def load_dataset(train_df, validate=False):\n",
        "\n",
        "    max_encoder_length = 24*7*ENCODER_LENGTH_IN_WEEKS # use 5 past weeks\n",
        "    max_prediction_length = 24*7 # to predict 1 week of future\n",
        "    training_cutoff = train_df[\"time_idx\"].max() - max_prediction_length\n",
        "\n",
        "    # build training dataset\n",
        "    tr_ds = TimeSeriesDataSet(\n",
        "        # with validate=False use all data\n",
        "        train_df[lambda x: x.time_idx <= training_cutoff] if validate else train_df,\n",
        "        time_idx=\"time_idx\",\n",
        "        target=\"target\",\n",
        "        group_ids=[\"num\"],\n",
        "        min_encoder_length=1,\n",
        "        max_encoder_length=max_encoder_length,\n",
        "        min_prediction_length=1,\n",
        "        max_prediction_length=max_prediction_length,\n",
        "        time_varying_known_categoricals=CATE_COLS,\n",
        "        static_categoricals=[\"num\", \"mgrp\", \"cluster\"],\n",
        "        time_varying_known_reals=[\n",
        "            \"time_idx\",\n",
        "            'hour',\n",
        "            \"temperature\",\n",
        "            \"windspeed\",\n",
        "            \"humidity\",\n",
        "            \"precipitation\",\n",
        "            \"insolation\",\n",
        "            'cumhol'\n",
        "        ],\n",
        "        target_normalizer=GroupNormalizer(groups=[\"num\"], transformation=\"softplus\"),\n",
        "        time_varying_unknown_categoricals=[],\n",
        "        time_varying_unknown_reals=[\n",
        "            \"target\",\n",
        "            \"log_target\",\n",
        "            \"mean_target\",\n",
        "            \"mean_target_num\",\n",
        "            \"mean_target_mgrp\",\n",
        "            \"mean_target_cluster\"\n",
        "        ],\n",
        "        add_relative_time_idx=True,  # add as feature\n",
        "        add_target_scales=True,  # add as feature\n",
        "        add_encoder_length=True,  # add as feature\n",
        "    )\n",
        "\n",
        "    va_ds = None\n",
        "    if validate:\n",
        "        # validation dataset not used for submission\n",
        "        va_ds = TimeSeriesDataSet.from_dataset(\n",
        "            tr_ds, train_df, predict=True, stop_randomization=True\n",
        "        )\n",
        "\n",
        "    return tr_ds, va_ds\n",
        "\n",
        "# training\n",
        "def fit(seed, tr_ds, va_loader=None):\n",
        "    seed_all(seed) # doesn't really work as training is non-deterministic\n",
        "\n",
        "    # create dataloaders for model\n",
        "    tr_loader = tr_ds.to_dataloader(\n",
        "        train=True, batch_size=BATCH_SIZE, num_workers=12\n",
        "    )\n",
        "\n",
        "    if va_loader is not None:\n",
        "        # stop training, when loss metric does not improve on validation set\n",
        "        early_stopping_callback = EarlyStopping(\n",
        "            monitor=\"val_loss\",\n",
        "            min_delta=1e-4,\n",
        "            patience=20,\n",
        "            verbose=True,\n",
        "            mode=\"min\"\n",
        "        )\n",
        "        lr_logger = LearningRateMonitor(logging_interval=\"epoch\")  # log the learning rate\n",
        "        callbacks = [lr_logger, early_stopping_callback]\n",
        "    else:\n",
        "        # gather 10 checkpoints with best traing loss\n",
        "        checkpoint_callback = ModelCheckpoint(\n",
        "            monitor='train_loss',\n",
        "            dirpath=CKPTROOT,\n",
        "            filename=f'seed={seed}'+'-{epoch:03d}-{train_loss:.2f}',\n",
        "            save_top_k=10\n",
        "        )\n",
        "        callbacks = [checkpoint_callback]\n",
        "\n",
        "    # create trainer\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=args.nepochs,\n",
        "        gpus=[args.gpu],\n",
        "        gradient_clip_val=PARAMS['gradient_clip_val'],\n",
        "        limit_train_batches=30,\n",
        "        callbacks=callbacks,\n",
        "        logger=TensorBoardLogger(LOGDIR)\n",
        "    )\n",
        "\n",
        "    # use pre-deterined leraning rate schedule for final submission\n",
        "    learning_rate = LRS if va_loader is None else PARAMS['learning_rate']\n",
        "\n",
        "    # initialise model with pre-determined hyperparameters\n",
        "    tft = TemporalFusionTransformer.from_dataset(\n",
        "        tr_ds,\n",
        "        learning_rate=learning_rate,\n",
        "        hidden_size=PARAMS['hidden_size'],\n",
        "        attention_head_size=PARAMS['attention_head_size'],\n",
        "        dropout=PARAMS['dropout'],\n",
        "        hidden_continuous_size=PARAMS['hidden_continuous_size'],\n",
        "        output_size=1,\n",
        "        loss=SMAPE(), # SMAPE loss\n",
        "        log_interval=10,  # log example every 10 batches\n",
        "        logging_metrics=[SMAPE()],\n",
        "        reduce_on_plateau_patience=4,  # reduce learning automatically\n",
        "    )\n",
        "    print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
        "\n",
        "    kwargs = {'train_dataloader': tr_loader}\n",
        "    if va_loader:\n",
        "        kwargs['val_dataloaders'] = va_loader\n",
        "\n",
        "    # fit network\n",
        "    trainer.fit(\n",
        "        tft,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    best_model_path = trainer.checkpoint_callback.best_model_path\n",
        "    print(f\"{best_model_path=}\")\n",
        "    best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
        "\n",
        "    return best_tft\n",
        "\n",
        "# predict 1 week\n",
        "def forecast(ckpt, train_df, test_df):\n",
        "    # load model\n",
        "    best_tft = TemporalFusionTransformer.load_from_checkpoint(ckpt)\n",
        "    max_encoder_length = best_tft.dataset_parameters['max_encoder_length']\n",
        "    max_prediction_length = best_tft.dataset_parameters['max_prediction_length']\n",
        "\n",
        "    assert max_encoder_length == 5*24*7 and max_prediction_length == 1*24*7\n",
        "\n",
        "    # use 5 weeks of training data at the end\n",
        "    encoder_data = train_df[lambda x: x.time_idx > x.time_idx.max() - max_encoder_length]\n",
        "\n",
        "    # get last entry from training data\n",
        "    last_data = train_df.iloc[[-1]]\n",
        "\n",
        "    # fill NA target value in test data with last values from the train dataset\n",
        "    target_cols = [c for c in test_df.columns if 'target' in c]\n",
        "    for c in target_cols:\n",
        "        test_df.loc[:, c] = last_data[c].item()\n",
        "\n",
        "    decoder_data = test_df\n",
        "\n",
        "    # combine encoder and decoder data. decoder data is to be predicted\n",
        "    new_prediction_data = pd.concat([encoder_data, decoder_data], ignore_index=True)\n",
        "    new_raw_predictions, new_x = best_tft.predict(new_prediction_data, mode=\"raw\", return_x=True)\n",
        "\n",
        "    # num_labels: mapping from 'num' categorical feature to index in new_raw_predictions['prediction']\n",
        "    #             {'5': 4, '6': 6, ...}\n",
        "    # new_raw_predictions['prediction'].shape = (60, 168, 1)\n",
        "    num_labels = best_tft.dataset_parameters['categorical_encoders']['num'].classes_\n",
        "\n",
        "    preds = new_raw_predictions['prediction'].squeeze()\n",
        "\n",
        "    sub_df = pd.read_csv(DATAROOT/\"sample_submission.csv\")\n",
        "\n",
        "    # get prediction for each building (num)\n",
        "    for n, ix in num_labels.items():\n",
        "        sub_df.loc[sub_df.num_date_time.str.startswith(f\"{n} \"), 'answer'] = preds[ix].numpy()\n",
        "\n",
        "    # save predction to a csv file\n",
        "    outfn = CSVROOT/(Path(ckpt).stem + '.csv')\n",
        "    print(outfn)\n",
        "    sub_df.to_csv(outfn, index=False)\n",
        "\n",
        "def ensemble(outfn):\n",
        "    # get all prediction csv files\n",
        "    fns = list(CSVROOT.glob(\"*.csv\"))\n",
        "    df0 = pd.read_csv(fns[0])\n",
        "    df = pd.concat([df0] + [pd.read_csv(fn).loc[:,'answer'] for fn in fns[1:]], axis=1)\n",
        "    # get median of all predcitions\n",
        "    df['median'] = df.iloc[:,1:].median(axis=1)\n",
        "    df = df[['num_date_time', 'median']]\n",
        "    df = df.rename({'median': 'answer'}, axis=1)\n",
        "    # save to submission file\n",
        "    df.to_csv(outfn, index=False)\n",
        "\n",
        "# not used for final submission\n",
        "def validate(seed, tr_ds, va_ds):\n",
        "    va_loader = va_ds.to_dataloader(\n",
        "        train=False, batch_size=BATCH_SIZE*10, num_workers=12\n",
        "    )\n",
        "    best_tft = fit(seed, tr_ds, va_loader)\n",
        "    actuals = torch.cat([y[0] for x, y in iter(va_loader)])\n",
        "    predictions = best_tft.predict(va_loader)\n",
        "    smape_per_num = SMAPE(reduction=\"none\")(predictions, actuals).mean(1)\n",
        "    print(smape_per_num)\n",
        "    print(smape_per_num.mean())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    [p.mkdir(exist_ok=True) for p in (CKPTROOT, CSVROOT, LOGDIR)]\n",
        "\n",
        "    train_df, test_df = prep()\n",
        "    tr_ds, va_ds = load_dataset(train_df, args.val)\n",
        "\n",
        "    if args.val:\n",
        "        validate(args.seed[0], tr_ds, va_ds)\n",
        "    else:\n",
        "        if args.fit:\n",
        "            print(\"### FIT ###\")\n",
        "            for s in args.seed:\n",
        "                fit(s, tr_ds)\n",
        "\n",
        "        if args.forecast:\n",
        "            print(\"### FORECAST ###\")\n",
        "            for p in CKPTROOT.glob(\"*.ckpt\"):\n",
        "                forecast(p, train_df, test_df)\n",
        "\n",
        "            print(\"### ENSEMBLING ###\")\n",
        "            ensemble(SUBFN)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}